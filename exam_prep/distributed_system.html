<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Exam Search: Distributed Systems</title>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- UIkit CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/css/uikit.min.css" />

    <!-- UIkit JS -->
    <script src="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/js/uikit.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/js/uikit-icons.min.js"></script>

    <link rel="stylesheet" href="main.css" />
</head>
<body>

<input id="search_bar" class="uk-input search" onkeyup="search()" type="text" name="search" placeholder="search...">
<div style="text-align: center;">
    <div class="uk-card uk-card-secondary uk-card-body content">
        <h3>A Definitive Guide For Distributed Computing</h3>
    </div>
    <div id="distributed systems definition introduction" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Definition of Distributed Systems</h3>
        </div>
        <div class="uk-card-body">
            A distributed system consists of a collection of
            autonomous computers, connected through a
            network and distribution middleware, which
            enables computers to coordinate their activities
            and to share the resources of the system, so that
            users perceive the system as a single, integrated
            computing facility.
        </div>
    </div>

    <div id="properties and common goals of distributed systems introduction" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Common Goals and properties of distributed systems</h3>
        </div>
        <div class="uk-card-body">
            A distributed system may have a common goal, such as solving a large computational
            problem.

            Alternatively, each computer may have its own user with individual needs, and the
            purpose of the distributed system is to coordinate the use of shared resources or
            provide communication services to the users.

            Other typical properties of distributed systems include the following:

            The system has to tolerate failures in individual computers.

            The structure of the system (network topology, network latency, number of computers)
            is not known in advance, the system may consist of different kinds of computers and
            network links, and the system may change during the execution of a distributed
            program.

            Each computer has only a limited, incomplete view of the system. Each computer may
            know only one part of the input.
        </div>
    </div>

    <div id="parallel and distributed computing characteristics" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Parallel and Distributed Computing</h3>
        </div>
        <div class="uk-card-body">
            In parallel computing, all
            processors may have access
            to a shared memory to
            exchange information
            between processors.

            <br><br>

            In distributed computing,
            each processor has its own
            private memory (distributed
            memory). Information is
            exchanged by passing
            messages between the
            processors.
        </div>
    </div>

    <div id="introduction to distributed systems system architecture user perception" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>System architecture</h3>
        </div>
        <div class="uk-card-body">
            the machines are autonomous; this means they are
            computers which, in principle, could work independently.
            <br><br>
            <b>User Perception:</b>
            the distributed system is perceived as a single system
            solving a certain problem (even though, in reality, we have several computers
            placed in different locations).

            The distributed system has following characteristics:

            They do not have share memory or clock

            The computers communicate between themselves by the exchanging messages
            over a communication network

            Each computer has its own memory and operating system
        </div>
    </div>

    <div id="advantages of distributed systems system" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Advantages of Distributed Systems</h3>
        </div>
        <div class="uk-card-body">
            Performance:
            very often a collection of processors can provide higher   performance (and better price/performance ratio) than a Centralized computer.
            <br><br>
            Distribution:
            many applications involve, by their nature, spatially separated machines banking, Commercial, automotive system).
            <br><br>
            Reliability (fault tolerance):
            if some of the machines crash, the system can survive.
            <br><br>
            Incremental growth:
            as requirements on processing power grow, new machines can be added incrementally.
            <br><br>
            Sharing of data/resources:
            shared data is essential to many applications (banking,  computer supported
            Cooperative work, reservation systems)
            other resources can be also shared (e.g. expensive printers).
            <br><br>
            Communication:
            facilitates human-to-human communication.

        </div>
    </div>

    <div id="disadvantages of distributed computing" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Disadvantages of distributed computing</h3>
        </div>
        <div class="uk-card-body">
            Difficulties of developing distributed software:
            how should operating systems, programming languages and applications look like?
            <br><br>
            Networking problems
            several problems are created by the network infrastructure, which have to be dealt with: loss of messages, overloading.
            <br><br>
            Security problems
            sharing generates the problem of data security.
        </div>
    </div>

    <div id="centralised and distributed computing characteristics" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Centralized & Distributed System characteristics</h3>
        </div>
        <div class="uk-card-body">
            <b>Centralised Computing</b><br>
            One component with non-autonomous Parts<br>
            Component shared by users all the time<br>
            All resources accessible<br>
            Software runs in a single process<br>
            Single Point of control<br>
            Single Point of failure
            <br><br>
            <b>Distributed Computing</b><br>
            Multiple autonomous components<br>
            Components are not shared by all users<br>
            Resources may not be accessible<br>
            Software runs in concurrent processes on different processors<br>
            Multiple Points of control<br>
            Multiple Points of failure

        </div>
    </div>

    <div id="what are we trying to achieve when we construct a distributed system resource sharing openness concurrency scalability fault tolerance transparency access location migration replication failure performance relocation persistence" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>What are we trying to achieve when we construct a distributed system?</h3>
        </div>
        <div class="uk-card-body">
            Certain common characteristics can be used to assess distributed systems<br><br>
            • <b>Resource Sharing</b><br>
            Ability to use any hardware, software or data anywhere in the system.<br>

            Resource manager controls access, provides naming scheme and controls concurrency.<br>

            Resource sharing model (e.g. client/server or object-based) describing how<br>
            - resources are provided,<br>
            - they are used and<br>
            - provider and user interact with each other.<br><br>

            • <b>Openness</b><br>
            Openness is concerned with extensions and improvements of distributed systems.<br>

            Detailed interfaces of components need to be published.<br>
            New components have to be integrated with existing components.<br>

            Differences in data representation of interface types on different processors (of different vendors) have to be resolved.<br><br>

            • <b>Concurrency</b><br>

            Components in distributed systems are executed in concurrent processes.<br>

            Components access and update shared resources (e.g. variables, databases, device drivers).<br>

            Integrity of the system may be violated if concurrent updates are not coordinated.<br>
            - Lost updates<br>
            - Inconsistent analysis<br><br>

            • <b>Scalability</b><br>
            Adaption of distributed systems to<br>
            - accommodate more users<br>
            - respond faster (this is the hard one)<br>

            Usually done by adding more and/or faster processors.<br>

            Components should not need to be changed when increases scale of a system.<br>

            Design components to be scalable!<br><br>

            • <b>Fault Tolerance</b><br>
            Hardware, software and networks fail!<br>

            Distributed systems must maintain availability even at low levels of hardware/software/network reliability.<br>

            Fault tolerance is achieved by<br>
            - recovery<br>
            - redundancy<br><br>

            • <b>Transparency</b><br>
            Distributed systems should be perceived by users and application programmers as a whole rather than as a collection of cooperating components.<br>

            Transparency has different dimensions.<br>

            These represent various properties that distributed systems should have.<br><br>

            - <b>Access Transparency:</b> Local and remote resources are accessed using identical operations.<br>
            - <b>Location Transparency:</b> Users are unaware of the location of resources<br>
            - <b>Migration Transparency:</b> Resources can migrate without name change<br>
            - <b>Replication Transparency:</b> Users are unaware of the existence of multiple copies of resources<br>
            - <b>Failure Transparency:</b> Users are unaware of the failure of individual components<br>
            - <b>Concurrency Transparency:</b> Users are unaware of sharing resources with others<br>
            - <b>Performance transparency:</b> load variation should not lead to performance degradation. This could be achieved by automatic reconfiguration as response to changes of the load; it is difficult to achieve.<br>
            - <b>Relocation Transparency:</b> Hide that a resource may be moved to another location while in use (the others don’t notice).<br>
            - <b>Persistence Transparency:</b> Hide whether a (software) resource is in memory or on disk<br>
        </div>
    </div>

    <div id="issues in distributed systems global knowledge naming scalability compatibility process synchronisation resource management security structuring" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Issues in distributed systems</h3>
        </div>
        <div class="uk-card-body">
            • <b>Global Knowledge</b><br>
            Due to the unavailability of global clock and shared memory and unpredictable message delay, it is impossible to to collect the upto date information about the global state<br><br>

            • <b>Naming</b><br>
            Names are used to refer the objects. A name  service map a logical name to the physical address by making use of lookup table<br><br>

            • <b>Scalability</b><br>
            Grow with time<br><br>

            • <b>Compatibility</b><br>
            Three level of compatibility exist in DS<br>
            - Binary level<br>
            - Execution level<br>
            - Protocol level<br><br>

            • <b>Process synchronization</b><br>
            The synchronization is DS is difficult because of the unavailable of shared memory
            Problem of mutual exclusion<br><br>

            • <b>Resource management</b><br>
            Resource management concern with both local and remote resources available to user in an effective manner<br><br>

            • <b>Security</b><br>
            Authentication and authorization<br><br>

            • <b>Structuring</b><br>

        </div>
    </div>

    <div id="system models model architectural application architecture middleware operating platform rpc rmi corba java microsoft dcom web services client server and peer to peer thin fat" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>System Models</h3>
        </div>
        <div class="uk-card-body">
            Systems that are intended for use in real world environment should be designed to function correctly in the widest possible range of circumstances and in the face of many possible difficulties and threats.<br><br>

            Earlier slides shows that DS of different types share important underlying properties and give rise to design problems.<br><br>

            Here we are going to discuss common properties and design issues for distributed systems in the form of descriptive models.<br><br>

            Each model is intended to provide an abstract, simplified but consistent description of a relevant aspect of distributed system design.<br><br>

            • <b>Architectural Model</b><br>
            defines the way in which the component of system interact with each another and the way they have been implemented in the network.<br><br>

            These models are basically concern with the placement of system and its part and the relationship that exist between them. The goal of architecture model is to meet the current needs as well as the need of future<br><br>
            - <b>Software Architecture</b><br>
            It refers to the structuring of software as layers or modules in a single computer and in terms of services offered and requested between processes located in the same or different computers. These process and service oriented views can be expressed as service layers.<br>
            <pre>
                Application services
                middle ware
                operating system ____________platform
                computer & nw h/w ______|
            </pre>
            <b>Platform</b><br>
            The lowest level hardware and software layers are referred as platform to distributed system and their application.<br>

            These low level layers provide services to the layers above them.<br>

            Intel x86/ Windows, Intel x86/Solarsis, Intel x86/Linux, PowerPC/Mac OS X are major example of hardware and software layers.<br><br>

            <b>Middle Ware</b><br>
            It is layer of software whose purpose is to mask heterogeneity and to provide a convenient programming model to application programmers.<br>
            The goal of middleware is to create system independent interfaces for distributed applications.<br>
            The principle aim of middleware, namely raising the level of abstraction for distributed programming, is achieved in three ways.<br><br>

            communication mechanisms that are more convenient and less error prone than basic message passing;<br>
            independence from OS, network protocol, programming language, etc. and<br>
            standard services (such as a naming service, transaction service, security service, etc.).<br><br>

            To make the integration of these various services easier, and to improve transparency and system independence, middleware is usually based on a particular paradigm, or model, for describing distribution and communication.<br><br>

            <b>RPC</b> (Sun RPC) and group communication systems such as Isis were amongst the earliest instances of middleware. Object-oriented middleware products and standards are widely used, such as<br>
            <b>Java RMI</b> (Remote Method Invocation)<br>
            <b>CORBA</b> (Common Object Request Broker Architecture)<br>
            Web services<br>
            <b>Microsoft DCOM</b> (Distributed Component Object Model)<br><br>


            - <b>System Architecture</b><br>

            A distributed system is composed of a number of elements, the most important of which are software components, processing nodes and networks.<br>

            Some of these elements can be specified as part of a distributed system’s design, while others are given.<br>

            Typically when building a distributed system, the software is under the designer’s control.<br>

            Depending on the scale of the system, the hardware can be specified within the design as well, or already exists and has to be taken as-is.<br>

            The key, however, is that the software components must be distributed over the hardware components in some way.<br><br>

            The software of distributed systems can become fairly complex—especially in large distributed systems—and its components can spread over many machines.<br>
            It is important, therefore, to understand how to organize the system.<br>
            The software architecture of distributed systems deals with how software components are organized and how they work together, i.e., communicate with each other.<br>
            Once the software components are instantiated and placed on real machines, then the actual system architecture comes into picture.<br>
            <b>Client Server</b> and <b>Peer to Peer</b><br><br>

            <b>Client Server Architecture</b><br>
            The client-server architecture is the most common and widely used model for communication between processes.<br>
            In this architecture one process takes on the role of a server, while all other processes take on the roles of clients.<br>
            The server process provides a service (e.g., a time service, a database service, a banking service, etc.) and the clients are customers of that service.<br>
            A client sends a request to a server, the request is processed at the server and a reply is returned to the client.<br>
            A typical client-server application can be decomposed into three logical parts: the interface part, the application logic part, and the data part.<br><br>

            Implementations of the client-server architecture vary with regards to how the parts are separated over the client and server roles.<br>
            A <b>thin client</b> implementation will provide a minimal user interface layer, and leave everything else to the server.<br>
            A <b>fat client</b> implementation, on the other hand, will include all of the user interface and application logic in the client, and only rely on the server to store and provide access to data.<br>
            Implementations in between will split up the interface or application logic parts over the clients and server in different ways.<br><br>

        </div>
    </div>

    <div id="vertical distribution multi tier" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Vertical Distribution</h3>
        </div>
        <div class="uk-card-body">
            An extension of the client-server architecture, the vertical distribution, or multi-tier, architecture distributes the traditional server functionality over multiple servers.<br>
            A client request is sent to the first server.<br>
            During processing of the request this server will request the services of the next server, who will do the same, until the final server is reached.<br>
            In this way the various servers become clients of each other. Each server is responsible for a different step (or tier) in the fulfillment of the original client request.<br>

        </div>
    </div>

    <div id="horizontal distribution multi tier" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Horizontal Distribution</h3>
        </div>
        <div class="uk-card-body">
            While vertical distribution focuses on splitting up a server’s functionality over multiple computers, horizontal distribution involves replicating a server’s functionality over multiple computers.<br>
            A typical example, as shown in Figure, is a replicated Web server.<br>
            In this case each server machine contains a complete copy of all hosted Web pages and client requests are passed on to the servers in a round robin fashion.<br>
            The horizontal distribution architecture is generally used to improve scalability (by reducing the load on individual servers) and reliability (by providing redundancy).<br>

        </div>
    </div>

    <div id="peer to peer" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Peer to Peer</h3>
        </div>
        <div class="uk-card-body">
            The peer to peer (P2P) architecture takes the opposite approach and assumes that all processes play the same role, and are therefore peers of each other.<br>
            In this architecture, each process acts as both client and server, both sending out requests and processing incoming requests. The P2P model all processes provide the same logical services.<br>
            Well known examples of the P2P model are file-sharing applications.<br>
            When a node wishes to send a message to an arbitrary other node, in this architecture it must first locate that node by propagating a request along the links in the overlay network. Once the destination node is found, the two nodes can typically communicate directly.<br><br>

            There are two key types of overlay networks, the distinction being based on how they are built and maintained. In all cases a node in the network will maintain a list of neighbors.<br>
            In unstructured overlays the structure of the network often resembles a random graph. In order to keep the network connected as nodes join and leave, all nodes periodically exchange their partial views with neighbors, creating a new neighbor list for themselves. As long as nodes both push and pull this information the network tends to stay well connected.<br><br>

            In the case of structured overlays the choice of a node’s neighbors is determined according to a specific structure.<br>
            In a distributed hash table, for example, nodes work together to implement a hash table. Each node is responsible for storing the data associated with a range of identifiers.<br>
            When joining a network, a node is assigned an identifier, locates the node responsible for the range containing that identifier, and takes over part of that identifier space.<br>
            Each node keeps track of its neighbors in the identifier space.<br>

        </div>
    </div>

    <div id="fundamental model interaction synchronous asynchronous arbitrary failures byzantine timing security" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Fundamental Model</h3>
        </div>
        <div class="uk-card-body">
            • <b>Interaction Model</b><br>
            It describe how processes coordinate their actions.<br>
            The process interacts within process by passing messages, resulting in communication flow and coordination of process.<br>
            The rate at which each process proceed and the timing of the transmission of messages cannot be predicated.<br>
            The interacting process performs all the activity in a distributed system.<br>
            Each process has its own state, consisting of the set of data that it can access and update.<br>
            The state belonging to each process is completely private, i.e. it cannot be accessed or updated by another process.<br>
            In a distributed system it is hard to set time limits on the time taken for execution by a process, or for message delivery or clock drift.<br><br>

            There are two interaction models:<br>
            - Synchronous Model<br>
            - Asynchronous Model<br>
            Synchronous Model is one in which following bounds are defined:<br>
            The time to execute each step of a process has known upper and lower bounds<br>
            Each message transmitted over a channel is received within a known bound time<br>
            Each process has a local clock whose drift rate from real time has a known bound<br>
            It is difficult to arrive at the realistic values of the bounds, message delays and clock drift in the distributed system and to provide the guarantee over these chosen values.<br><br>

            Unless these values cannot be guaranteed, their reliability will be in question. However, in a synchronous model it is possible to use timeouts.<br>
            Asynchronous Models are one in which there is no bounds on:<br>
            Process execution speed<br>
            Message transmission delay<br>
            Clock drift rates<br>
            The asynchronous model allows no assumption about the time intervals involved in any execution. Actual distributed systems are very often asynchronous because of the need for processes to share the processors and for communication channels to share the network. If the process execution speed is not known and the process is sharing too many processor, than the nature of the process will be asynchronous.<br><br>

            • <b>Failure Model</b><br>
            Failure Model defines the ways in which failure may occur in order to provide an understanding of the effects of failures. In distributed system, both process and communication channel may fail; these failures are listed in different categories:<br>
            Omission Failure refers to cases when a process or communication channel fails to perform action that it is supposed to do.<br>
            Process Omission Failure If a process is crashed i.e. it has halted and will not be executed any further. Such process called as fail-stop if other process can detect certainly that the process has crashed.<br>
            Communication Omission Failure The communication channel produces an omission failure if it is not able to transport message from sender to receiver’s buffer memory. This is known as buffer memory and is generally caused by lack of space in the buffer memory.<br><br>

            - <b>Arbitrary Failures or Byzantine Failure:</b> it is used to describe possible failure semantics, in which any type of error may occur.<br>
            An arbitrary failure of a process is one in which it arbitrarily omits intended processing steps or takes unintended processing steps.<br>
            These failures cannot be detected by using whether the process responds to invocation.<br>
            Arbitrary failures of communication channels are rare as the communication software is able to detect and remove faulty messages.<br><br>

            - <b>Timing Failures</b> Timing failures are applicable in synchronous distributed systems where time limits are set on process execution time, message delivery time and clock drift rate.<br>
            In asynchronous distributed system if the responses from the server take time it cannot be counted as timing failure.<br>
            The timing failures are normally limited to synchronous systems where the bounds are known and therefore it could be assessed whether the failure has occurred or not.<br><br>


            • <b>Security Model</b><br>
            The security of a distributed system can be achieved by securing the processes and the channels used for their interactions and by protecting the objects that they encapsulate against unauthorized access.<br>

            The threats from a potential enemy are threat to processors, threat to communication channels and denial of service. The system can be secured from these threats by the technique of cryptography.<br>

            Protection is described in terms of objects and resources<br>


        </div>
    </div>

    <div id="inherent limitations of distributed systems absence of global time shared memory" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Inherent limitations of Distributed Systems</h3>
        </div>
        <div class="uk-card-body">
            <b>Absence of Global Clock</b><br>
            In DS there is no system wide common clock or global clock. Notion of global time does not exist.<br>
            Suppose a common clock is available for all process in the system: then two different process can observe a global clock value at different instant due to unpredictable message transmission delay.<br>
            On other hand, if we provide each computer in the system with physical clock, these physical clock can drift from physical time.<br><br>

            <b>IMPACT OF THE ABSENCE OF GLOBAL TIME</b><br>
            The concept of temporal ordering is integral to design and development of any DS.<br>
            For example an OS is responsible for scheduling process on the basis of temporal ordering.<br>
            Due to the absence of global time, it is difficult to reason about temporal events in DS. Hence algorithm for DS are more difficult to design and debug compare to centralized system.<br>
            Due to absence of global clock it is hard to collect up-to-date information on state of system.<br><br>

            <b>ABSENCE OF SHARED MEMORY</b><br>
            Computer in DS do not share memory, an up to date state of entire system is not available to any process.<br>
            A process in DS can obtain coherent but partial view of system or a complete but incoherent view of system.<br>
            Coherent view means all observation of different process are made at the same physical time.<br>
            Complete view encompasses the local state of all computers and any message in transit in DS i.e. Global State.<br>
        </div>
    </div>

    <div id="theoretical aspects" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Theoretical Aspects</h3>
        </div>
        <div class="uk-card-body">
            Logical Clocks<br>
            Causal Ordering<br>
            Global State Recording<br>
            Termination Detection<br>
        </div>
    </div>

    <div id="lamport's lamports clock" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Lamport's Clock</h3>
        </div>
        <div class="uk-card-body">
            Happened before relation:<br>
            - a -> b : Event a occurred before event b. Events in the same process.<br>
            - a -> b : If a is the event of sending a message m in a process and  b is the event of receipt of the same message m by another process.<br>
            - a -> b, b -> c, then a -> c. “->” is transitive.<br>
            Causally Ordered Events<br>
            - a -> b : Event a “causally” affects event b<br>
            Concurrent Events<br>
            - a || b: if a !-> b and b !-> a<br>
        </div>
    </div>

    <div id="logical clocks" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Logical Clocks</h3>
        </div>
        <div class="uk-card-body">
            Conditions satisfied:<br>
            - Ci is clock in Process Pi.<br>
            - If a -> b in process Pi, Ci(a) < Ci(b)<br>
            - Let a: sending message m in Pi; b : receiving message m in Pj; then, Ci(a) < Cj(b).<br>
            Implementation Rules:<br>
            - R1: Ci = Ci + d (d > 0); clock is updated between two successive events.<br>
            - R2: Cj = max(Cj, tm + d); (d > 0);  When Pj receives a message m with a time stamp tm (tm assigned by Pi, the sender; tm = Ci(a), a being the event of sending message m).<br>
            A reasonable value for d is 1<br>

        </div>
    </div>

    <div id="vector clocks comparisons" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Vector clocks</h3>
        </div>
        <div class="uk-card-body">
            Keep track of transitive dependencies among processes for recovery purposes.<br>
            Ci[1..n]: is a “vector” clock at process Pi whose entries are the “assumed”/”best guess” clock values of different processes.<br>
            Ci[j] (j != i) is the best guess of Pi for Pj’s clock.<br>
            Vector clock rules:<br>
            - Ci[i] = Ci[i] + d,  (d > 0); for successive events in Pi<br>
            - For all k, Cj[k] = max (Cj[k],tm[k]), when a message m with time stamp tm is received by Pj from Pi.<br><br>

            Vector Clocks Comparisons<br>
            1.  Equal:  ta = tb  iff<br>
            2.  Not Equal: ta != tb  iff ta[i] != tb[i], for at least one i<br>
            3.  Less than or equal:  ta <= tb  iff ta[i] <= tb[i], for all i<br>
            4.  Less than :  ta < tb  iff ta[i] <= tb[i] and ta[i] != tb[i], for all i<br>
            5.  Concurrent:  ta || tb  iff ta  !< tb  and tb !< ta<br>
            6.  Not less than or equal ...<br>
            7.  Not less than ..<br>

        </div>
    </div>

    <div id="global state recording" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Global State</h3>
        </div>
        <div class="uk-card-body">
            A global state, GS of a system is the collection of the local states of its sites<br>
            i.e. GS=(LS1,Ls2,LS3,----LSn) where n is the number of sites in the system<br><br>

            RECORDING GLOBAL STATE:<br>
            (e.g.,) Global state of A is recorded in (1) and not in (2).<br>
            - State of B, C1, and C2 are recorded in (2)<br>
            - Extra amount of $50 will appear in global state<br>
            - Reason: A’s state recorded before sending message and C1’s state after sending message.<br>
            Inconsistent global state if n < n’, where<br>
            - n is number of messages sent by A along channel before A’s state was recorded<br>
            - n’ is number of messages sent by A along the channel before channel’s state was recorded.<br>
            Consistent global state: n = n’<br>

        </div>
    </div>

    <div id="chandy lamport algorithm chandy-lamport" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Chandy-Lamport algorithm</h3>
        </div>
        <div class="uk-card-body">
            The Chandy-Lamport algorithm uses a control message, called a Marker whose role in a FIFO system is to separate messages in the channels.<br>
            After a site has recorded its snapshot, it sends a marker, along all of its outgoing channels before sending out any more messages.<br>
            A marker separates the messages in the channel into those to be included in the snapshot from those not to be recorded in the snapshot.<br>
            A process must record its snapshot no later than when it receives a marker on any of its incoming channels.<br><br>
            The algorithm can be initiated by any process by executing the “Marker Sending Rule” by which it records its local state and sends a marker on each outgoing channel.<br>
            A process executes the “Marker Receiving Rule” on receiving a marker. If the process has not yet recorded its local state, it records the state of the channel on which the marker is received as empty and executes the “Marker Sending Rule” to record its local state.<br>
            The algorithm terminates after each process has received a marker on all of its incoming channels.<br>
            All the local snapshots get disseminated to all other processes and all the processes can determine the global state.<br><br>
            Marker Sending Rule for process I<br>
            1 Process i records its state.<br>
            2 For each outgoing channel C on which a marker has not been sent, i sends a marker along C before i sends further messages along C.<br>
            Marker Receiving Rule for process j<br>
            On receiving a marker along channel C: if j has not recorded its state then Record the state of C as the empty set Follow the “Marker Sending Rule” else Record the state of C as the set of messages received along C after j ’s state was recorded and before j received the marker along C<br>


        </div>
    </div>

    <div id="sun network file system sun nfs port mapper rpc access transparency virtual file system vfs hard soft mounted caching server validity condition write operation" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Sun Network File system</h3>
        </div>
        <div class="uk-card-body">
            NFS was the first file service that was designed as a product by sun Microsystem in 1985.<br><br>

            NFS provides transparent access to remote files for client programs running on UNIX and other system.<br><br>

            The client-server relationship is symmetrical: each computer in an NFS network can act as both a client and a server, and the files at every machine can be made available for remote access by other machines.<br><br>

            The NFS client and server modules communicate using remote procedure calling Sun’s RPC system.<br>
            It can be configured to use either UDP or TCP, and the NFS protocol is compatible with both.<br><br>

            A <b>port mapper</b> service is included to enable clients to bind to the service in a given host by name.<br><br>

            NFS provides <b>access transparency</b> user programs can issue file operations for local or remote files without distinction.<br><br>

            The integration is achieved by a <b>virtual file system (VFS)</b> module; which has been added to the UNIX kernel to distinguish between local and remote files.<br>

            In addition, VFS keeps track of the file systems that are currently available both locally and remotely, and it passes each request to the appropriate local system module<br><br>

            Remote file systems may be <b>hard mounted</b> or <b>soft mounted</b> in a  client computer.<br>

            <b>Hard Mounted:</b> In a hard mounted file system, If a user level process access a file  in remote file system then the process is suspended until the request can be completed and if the remote host is unavailable , the NFS client continues to retry to retry the request until it is satisfied<br>

            <b>Soft Mounted:</b> The client module will return a failure after a number of tries.<br>

            <b>Caching</b> in both the client  and the server computer are indispensable features of NFS implementations in order to achieve adequate performance.<br><br>

            <b>SERVER CACHING</b><br>
            NFS servers use the cache at the server machine just as it is used for other file accesses.<br>
            To ensure persistence of data, at the time of crash, the NFS protocol uses two options for write operation:<br>
            Write through Caching: Data in write operation received from clients is stored in the memory cache at the server and written to disk before a reply is sent to the client.<br>
            Data on write operation stored only in the memory cache. It will be written to disk when a commit operation is received from the relevant file.<br>
            The NFS client caches the results of operation in order to reduce the number of requests transmitted to servers.,<br>

            Client caching introduces the potential for different versions of files  to exist in different client nodes.<br>
            The clients are therefore, responsible for polling the server to check the currency of the cached data that they hold.<br>

            A timestamp based method is used to validate cached blocks before they are used.<br><br>

            <b>Validity condition</b><br>
            (T – Tc < t) ˅ (T<sub>mclient</sub> = T<sub>mserver</sub>)<br><br>

            <b>Write Operation</b><br>
            When a  cached page is modified it is marked as dirty and is scheduled to be flushed to the server asynchronously.<br>
            The modified pages are flushed when the file is closed or a sync occurs at the client<br>
        </div>
    </div>

    <div id="andrew file system benefits drawbacks call back mechanism information sharing on a large scale whole-file whole file caching serving architecture venus ufid representation of fids cells volumes tokens cache manager protection space design" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Andrew File System</h3>
        </div>
        <div class="uk-card-body">
            The design of the Andrew file system reflects an intention to support information sharing on a large scale by minimizing client - server communication.<br>
            This was achieved by transferring whole files between server and client computers and catching them at clients until the server receives a more up to date version.<br>
            AFS is distributed file system that enables co-operating hosts (clients and servers) to efficiently share file system resources across both local area and wide area networks<br>
            AFS provides transparent file access<br>
            AFS runs on systems from: HP, Next, DEC, IBM, SUN, and SGI.<br><br>
            AFS is based on a distributed file system originally developed at the Information Technology Center at Carnegie-Mellon University in 1984.<br>
            The idea was to provide a campus-wide file system for home directories which would run effectively using a limited bandwidth campus backbone network.<br><br>
            <b>whole-file serving:</b> entire contents of directories and files transferred from server to client (AFS-3: in chunks of 64 Kbytes)<br>
            <b>whole file caching:</b> when file transferred to client it will be stored on that client’s local disk<br><br>

            <b>ANDREW FILE SYSTEM ARCHITECTURE</b><br>
            Venus manages the cache removing the least recently used files when a new file is acquired from  a server to make the required space if the partition is full.<br>
            A flat file service is implemented by the Vice servers and the hierarchic directory structure required by UNIX  user programs is implemented by the set of Venus processes in the workstations<br>
            Each file and directory in the shared file space is identified by a unique, 96-bit file identifier (fid) similar to a UFID.<br>
            The Venus processes translate the pathnames issued by the clients to fids<br><br>

            The <b>representation of fids</b> include<br>
            Volume number:  represents the volume containing the file<br>
            NFS file handle: identifying file within volume<br>
            Uniquifier: for unique file identifiers<br><br>

            Files are grouped  into volumes for ease of location and movement.<br>
            • <b>Cells</b><br>
            - An AFS cell is a collection of servers grouped together administratively and presenting a single, cohesive file system.<br>
            - Typically, an AFS cell is a set of hosts that use the same Internet domain name.<br>
            - Normally, a variation of the domain name is used as the cell name. Users log into AFS client workstations which request information and files from the cell's servers on behalf of the users.<br><br>

            • <b>Volumes</b><br>
            - The storage disks in a computer are divided into sections called partitions. AFS further divides partitions into units called volumes.<br>
            - The volumes provide a convenient container for storing related files and directories.<br>
            - System administrator can move volumes from one file server to another without noticing, because AFS automatically tracks a volume’s location<br><br>

            • <b>Tokens</b><br>
            - AFS does not use UNIX user IDs for authentication.<br>
            - In order to access files which are not world accessible using AFS, you must have a valid AFS token. User may see what tokens currently they hold using the tokens command.<br><br>

            • <b>Cache Manager</b><br>
            - Cache Manager maintains information about the identities of the users logged into the machine, finds and requests data on their behalf, and keeps chunks of retrieved files on local disk.<br>
            - The effect of this is that as soon as a remote file is accessed a chunk of that file gets copied to local disk and so subsequent accesses (warm reads) are almost as fast as to local disk and considerably faster than a cold read (across the network).<br><br>

            • <b>File Protection</b><br>
            • <b>File Space Design</b><br><br>

            <b>BENEFITS</b><br>
            • Caching facility:    Caching significantly reduces the amount of network traffic, improving performance when a cold read is necessary<br>
            • Location Independence: AFS does its mapping (filename to location) at the server. This has the tremendous advantage of making the served file space location independent<br>
            • Scalability:     An architectural goal of the AFS designers was client/server ratios of 200:1 which has been successfully exceeded at some sites.<br>
            • Single Systems Image (SSI):     Establishing the same view of file store from each client and server in a network of systems (that comprise an AFS cell) is an order of magnitude simpler with AFS than it is with, say, NFS.<br>
            • Improved security: Firstly, AFS makes use of Kerberos to authenticate users. This improves security. Secondly, AFS uses access control lists (ACLs) to enable users to restrict access to their own directories.<br>
            • "Easy to use" networking<br>
            • Accessing remote file resources via the network becomes much simpler when using AFS<br>
            • Improved system management capability<br>
            • Systems administrators are able to make configuration changes from any client in the AFS cell<br>
            • Improved robustness to server crash<br>
            • Replicated AFS volumes<br><br>

            <b>DRAWBACKS</b><br>
            • Invasive install<br>
            • Complexity of backend server function<br>
            • Authentication issues with applications (e.g. ticket expiration)<br><br>

            <b>CALL BACK MECHANISM</b><br>
            • It ensures that cached copies of files are updated when another client performs<br>
            • a close operation on that file<br>
            • callback promise<br>
            – a token issued by Vice and stored with cached file<br>
            – status: valid or cancelled<br>
            • When server performs request to update file, then it<br>
            – sends callback to all Venus processes to which it has sent callback promise<br>
            – RPC from server to Venus process<br>
            – Venus process sets callback promise for local copy to cancelled<br>
            • Venus handling an open<br>
            – check whether local copy of file has valid callback promise<br>
            – if canceled, fresh copy must be fetched from Vice server<br>
            • Restart of workstation after failure<br>
            – retain as many locally cached files as possible, but callbacks may have been missed<br>
            – Venus sends cache validation request to the Vice server contains file modification timestamp<br>
            • if timestamp is current, server sends valid and callback promise is restored with valid<br>
            • if timestamp not current, server sends cancelled<br>
            • Problem: communication link failures<br>
            – callback must be renewed with above protocol before new open if a time T has lapsed since file was cached or callback promise was last validated<br>

        </div>
    </div>

    <div id="rpc remote procedure call local procedure vs strawman solution 4 properties marshaling port mapper" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>RPC (Remote Procedure Call)</h3>
        </div>
        <div class="uk-card-body">
            • A remote procedure call makes a call to a remote service look like a local call<br>
            - RPC makes transparent whether server is local or remote<br>
            - RPC allows applications to become distributed transparently<br>
            - RPC makes architecture of remote machine transparent<br>
            • An RPC is really a process invocation, not a procedure call.<br>
            • They make life easy for programmers, but hard for Network OS designers.<br><br>

            4 properties of distributed computing that make achieving transparency difficult:<br>
            • Partial failures<br>
            • Concurrency<br>
            • Latency<br>
            • Memory access<br><br>

            <b>STRAWMAN SOLUTION</b><br>
            Make remote behavior identical to local behavior:<br>
            Every partial failure results in complete failure<br>
            You abort and reboot the whole system<br>
            You wait patiently until system is repaired<br><br>

            <b>MARSHALLING</b><br>
            The client calls a local procedure, called the client stub. To the client process, it appears that this is the actual procedure.<br>
            The client stub packages the arguments to the remote procedure and builds one or more network messages.<br>
            The packaging of arguments into a network message is called marshaling.<br><br>

            <b>RPC PORT MAPPER</b><br>
            RPC Programs do not bind to specific protocol ports like several other technologies.<br>
            Instead, they use a Portmapper, which dynamically allocates an arbitrary unused protocol port for a connection.<br>
            An application does need to know what port to use to contact the Portmapper, but that is usually standardized in a company.<br>

        </div>
    </div>

    <div id="corba objects architecture orb idl dynamic invocation interface repository adapters" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>CORBA</h3>
        </div>
        <div class="uk-card-body">
            • CORBA is an industry standard developed by the OMG (a group of more than 700 companies) to aid in distributed objects programming.<br>
            • CORBA is just a specification for creating and using distributed objects;<br>
            • CORBA is not a programming language.<br>
            • The CORBA architecture is based on the object model.<br>
            • The model is abstract in the sense that it is not directly realized by any particular technology; this allows applications to be built in a standard manner using basic building blocks such as objects. Therefore, a CORBA-based system is a collection of objects that isolates the requestors of services (clients) from the providers of services (servers) by a well-defined encapsulating interface.<br><br>

            <b>CORBA OBJECTS</b><br>
            • CORBA objects can run on any platform.<br>
            • CORBA objects can be located anywhere on the network.<br>
            • CORBA objects can be written in any language that has IDL mapping.<br><br>

            <b>CORBA ARCHITECTURE</b><br>
            • CORBA is composed of five major components<br>
            • ORB,<br>
            • IDL,<br>
            • dynamic invocation interface(DII),<br>
            • interface repositories (IR), and<br>
            • object adapters (OA).<br><br>

            <b>ORB</b><br>
            • The ORB, which is the heart of CORBA, is responsible for all the mechanisms required to perform these tasks:<br>
            • Find the object implementation for the request.<br>
            • Prepare the object implementation to receive the request.<br>
            • Communicate the data making up the request.<br><br>

            <b>IDL</b><br>
            The IDL defines the types of objects by defining their interfaces. An interface consists of a set of named operations and the parameters to those operations. Note that IDL is used to describe interfaces only, not implementations. Despite the fact that IDL syntax is similar to C++
            and Java, IDL is not a programming language.<br><br>

            <b>Dynamic invocation interface</b><br>
            The DII, on the other hand, allows client applications to use server objects without knowing the type of those objects at compile time<br><br>

            <b>Interface Repository</b><br>
            The IR provides another way to specify the interfaces to objects. Interfaces can be added to the interface repository service. Using the IR, a client should be able to locate an object that is unknown at compile time, find information about its interface, then build a request to be forwarded through the ORB.<br><br>

            <b>Object Adapters</b><br>
            An object adapter is the primary way that an object implementation accesses services provided by the ORB. Such services include object reference generation and interpretation, method invocation, security of interactions, and object and implementation activation and deactivation.

        </div>
    </div>

    <div id="recovery classification of failures process system secondary communication" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Recovery in Distributed Systems</h3>
        </div>
        <div class="uk-card-body">
            • Failure of a site/node in a distributed system causes inconsistencies in the state of the system.<br>
            • Recovery: bringing back the failed node in step with other nodes in the system.<br>

            <b>Classification of Failures:</b><br>
            • Process failure:<br>
            - Deadlocks, protection violation, erroneous user input, etc.<br><br>

            • System failure:<br>
            - Failure of processor/system. System failure can have full/partial amnesia.<br>
            - It can be a pause failure (system restarts at the same state it was in before the crash) or a complete halt.<br><br>

            • Secondary storage failure:<br>
            - data inaccessible.<br><br>

            • Communication failure:<br>
            - network inaccessible.<br><br>

        </div>
    </div>

    <div id="backward and forward recovery problems" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Backward & Forward Recovery</h3>
        </div>
        <div class="uk-card-body">
            <b>Forward Recovery:</b><br>
            • Assess damages that could be caused by faults, remove those damages (errors), and help processes continue.<br>
            • Difficult to do forward assessment. Generally tough.<br><br>

            <b>Backward Recovery:</b><br>
            • When forward assessment not possible. Restore processes to previous error-free state.<br>
            • Expensive to rollback states<br>
            • Does not eliminate same fault occurring again (i.e. loop on a fault + recovery)<br>
            • Unrecoverable actions: print outs, cash dispensed at ATMs.<br><br>

            <b>Problems with Backward Error Recovery Approach</b><br>
            • The major problems associated with the backward error recovery approach are:<br>
            • Performance Penalty : The overhead to restore a process state to a prior state can be quite high.<br>
            • There is no guarantee that faults will not occur again when processing begins from a prior state.<br>
            • Some component of the system state may be recoverable.<br>
            • The forward error recovery technique, incur less overhead because only those parts of the state that deviate from the intended value need to be corrected.<br>
        </div>
    </div>

    <div id="recovery system model backward" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Recovery System Model</h3>
        </div>
        <div class="uk-card-body">
            • System is Single Machine, consist of Stable storage and secondary storage.<br>
            • Storage that does not lose information in event of system failure is stable storage.<br>
            • Stable storage is used to store the logs and recovery points.<br>
            • It is assumed that data on the secondary storage is archived periodically.<br><br>

            <b>For Backward Recovery</b><br>
            • Backward is simpler than forward as it  dependent of fault and error caused by fault.<br>
            • A single system with secondary and stable storage<br>
            • Stable storage does not lose information on failures<br>
            • Stable storage used for logs and recovery points<br>
            • Stable storage assumed to be more secure than secondary storage.<br>
            • Data on secondary storage assumed to be archived periodically.<br>
        </div>
    </div>

    <div id="recovery approaches" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Approaches</h3>
        </div>
        <div class="uk-card-body">
            <b>Operation-based Approach</b><br>
            • Maintaining logs: all modifications to the state of a process are recorded in sufficient detail so that a previous state can be restored by reversing all changes made to the state.<br>
            • (e.g.,) Commit in database transactions: a transaction if it is committed to by all nodes, then the changes are permanent. If it does not commit, the effect of transactions are to be undone.<br>
            • Updating-in-place: Every write (update) results in a log of (1) object name (2) old object state (3) new state. Operations:<br>
            • A do operation updates & writes the log<br>
            • An undo operation uses the log to remove the effect of a do<br>
            • A redo operation uses the log to repeat a do<br>
            • Write-ahead-log: To avoid the problem of a crash after update and before logging.<br>
            • Write (undo & redo) logs before update<br><br>

            <b>State-based Approach</b><br>
            • Establish a recovery point where the process state is saved.<br>
            • Recovery done by restoring the process state at the recovery, called a checkpoint. This process is called rollback.<br>
            • Process of saving called checkpointing or taking a check point.<br>
            • Rollback normally done to the most recent checkpoint, hence many checkpoints are done over the execution of a process.<br>
            • Shadow pages technique can be used for checkpointing. Page containing the object to be updated is duplicated and maintained as a checkpoint in stable storage.<br>
            • Actual update done on page in secondary storage. Copy in stable storage used for rollback.<br>
        </div>
    </div>

    <div id="recovery in concurrent systems" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Recovery in Concurrent Systems</h3>
        </div>
        <div class="uk-card-body">
            • Distributed system state involves message exchanges.<br>
            • In distributed systems, rolling back one process can cause the roll back of other processes.<br>
            • Orphan messages & the Domino effect: Assume Y fails after sending m.<br>
            • X has record of m at x3 but Y has no record. m -> orphan message.<br>
            • Y rolls back to y2 -> X should go to x2.<br>
            • If Z rolls back, X and Y has to go to x1 and y1 -> Domino effect, roll back of one process causes one or more processes to roll back.<br>
        </div>
    </div>

    <div id="lost messages" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Lost Messages</h3>
        </div>
        <div class="uk-card-body">
            • If Y fails after receiving m, it will rollback to y1.<br>
            • X will rollback to x1<br>
            • m will be a lost message as X has recorded it as sent and Y has no record of receiving it.<br>
        </div>
    </div>

    <div id="live locks livelocks" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Livelocks</h3>
        </div>
        <div class="uk-card-body">
            • Y crashes before receiving n1. Y rolls back to Y1 -> X to x1.<br>
            • Y recovers, receives n1 and sends m2.<br>
            • X recovers, sends n2 but has no record of sending n1<br>
            • Hence, Y is forced to rollback second time. X also rolls back as it has<br>
            • received m2 but Y has no record of m2.<br>
            • Above sequence can repeat indefinitely, causing a livelock.<br>
        </div>
    </div>

    <div id="consistent checkpoints" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Consistent Checkpoints</h3>
        </div>
        <div class="uk-card-body">
            • Overcoming domino effect and livelocks: checkpoints should not have messages in transit.<br>
            • Consistent checkpoints: no message exchange between any pair of processes in the set as well as outside the set during the interval spanned by checkpoints.<br>
            • {x1,y1,z1} is a strongly consistent checkpoint.<br>
        </div>
    </div>

    <div id="synchronous approach checkpointing first second phase" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Synchronous Approach</h3>
        </div>
        <div class="uk-card-body">
            <b>Checkpointing:</b><br>
            • First phase:<br>
            - An initiating process, Pi, takes a tentative checkpoint.<br>
            - Pi requests all other processes to take tentative checkpoints.<br>
            - Every process informs whether it was able to take checkpoint.<br>
            - A process can fail to take a checkpoint due to the nature of application (e.g.,) lack of log space, unrecoverable transactions.<br>
            • Second phase:<br>
            - If all processes took checkpoints, Pi decides to make the checkpoint permanent.<br>
            - Otherwise, checkpoints are to be discarded.<br>
            - Pi conveys this decision to all the processes as to whether checkpoints are to be made permanent or to be discarded.<br>
        </div>
    </div>

    <div id="distributed databases activity in issues data structures algorithms" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Distributed Databases</h3>
        </div>
        <div class="uk-card-body">
            Checkpointing objectives in distributed database systems (DDBS):<br>
            • Normal operations should be minimally interfered with, by checkpointing.<br>
            • A DDBS may update different objects in different sites, local checkpointing at each site is better.<br>
            • For faster recovery, checkpoints be consistent (desirable property).<br><br>
            Activity in DDBS is in terms of transactions. So in DDBS, a consistent checkpoint should either include updates of a transaction completely or not include it all.<br>
            Issues in identifying checkpoints:<br>
            • How sites agree on what transactions are to be included<br>
            • Taking checkpoints without interference<br><br>
            Data Structures<br>
            • LC: local clock as per Lamport’s logical clock<br>
            • LCPN (local checkpoint number): determined locally for the current checkpoint.<br><br>
            Algorithm: initiated by checkpoint coordinator (CC). CC uses checkpoint subordinates (CS).<br>
            • Phase 1 at the CC<br>
            • CC broadcasts a Checkpoint_Request message with a local timestamp LCcc.<br>
            • LCPNcc := LCcc<br>
            • CONVERTcc := false<br>
            • Wait for replies from CSs.<br>
            • Phase 1 at CSs<br>
        </div>
    </div>

    <div id="fault tolerance what is failure types of faults" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Fault Tolerance</h3>
        </div>
        <div class="uk-card-body">
            • Fault Tolerance is closely related to the notion of “Dependability”. In Distributed Systems, this is characterized under a number of headings:<br><br>

            • Availability – the system is ready to be used immediately.<br>
            • Reliability – the system can run continuously without failure.<br>
            • Safety – if a system fails, nothing catastrophic will happen.<br>
            • Maintainability – when a system fails, it can be repaired easily and quickly (and, sometimes, without its users noticing the failure).<br><br>

            <b>What is failure?</b><br>
            • A system is said to “fail” when it cannot meet its promises.<br>
            • A failure is brought about by the existence of “errors” in the system.<br>
            • The cause of an error is called a “fault”.<br><br>

            <b>Types of faults:</b>
            There are three main types of ‘fault’:<br><br>

            • Transient Fault – appears once, then disappears.<br>
            • Intermittent Fault – occurs, vanishes, reappears; but: follows no real pattern (worst kind).<br>
            • Permanent Fault – once it occurs, only the replacement/repair of a faulty component will allow the DS to function normally.<br><br>
        </div>
    </div>

    <div id="failure masking by redundancy information time physical" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Failure Masking by Redundancy</h3>
        </div>
        <div class="uk-card-body">
            Strategy: hide the occurrence of failure from other processes using redundancy.<br>

            Three main types:<br><br>

            • Information Redundancy – add extra bits to allow for error detection/recovery (e.g., Hamming codes and the like).<br>
            • Time Redundancy – perform operation and, if needs be, perform it again. Think about how transactions work (BEGIN/END/COMMIT/ABORT).<br>
            • Physical Redundancy – add extra (duplicate) hardware and/or software to the system.<br>
        </div>
    </div>

    <div id="system reliability fault intolerance vs fault tolerance issues process deaths machine failure network" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>System reliability: Fault-Intolerance vs. Fault-Tolerance</h3>
        </div>
        <div class="uk-card-body">
            The fault intolerance (or fault-avoidance) approach improves system reliability by removing the source of failures (i.e., hardware and software faults) before normal operation begins<br><br>

            The approach of fault-tolerance expect faults to be present during system operation, but employs design techniques which insure the continued correct execution of the computing process<br><br>

            <b>Issues</b><br>
            • Process Deaths:<br>
            - All resources allocated to a process must be recovered when a process dies<br>
            - Kernel and remaining processes can notify other cooperating processes<br>
            - Client-server systems: client (server) process needs to be informed that the corresponding server (client) process died<br><br>
            • Machine failure:<br>
            - All processes running on that machine will die<br>
            - Client-server systems: difficult to distinguish between a process and machine failure<br>
            - Issue: detection by processes of other machines<br><br>
            • Network Failure:<br>
            - Network may be partitioned into subnets<br>
            - Machines from different subnets cannot communicate<br>
            - Difficult for  a process to distinguish between a machine and a communication link failure<br>
        </div>
    </div>

    <div id="atomic actions" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Atomic Actions</h3>
        </div>
        <div class="uk-card-body">
            • Example: Processes P1 & P2 share a data named X.<br>
            - P1: ... lock(X); X:= X + Z; unlock(X); ...<br>
            - P2: ... lock(X); X := X + Y; unlock(X); ...<br><br>
            • Updating of X by P1 or P2 should be done atomically i.e., without any interruption.<br><br>
            • Atomic operation if:<br>
            - the process performing it is not aware of existence of any others.<br>
            - the process doing it does not communicate with others during the operation time.<br>
            - No other state change in the process except the operation.<br>
            - Effects on the system gives an impression of indivisible and perhaps instantaneous operation.<br>
        </div>
    </div>

    <div id="committing" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Committing</h3>
        </div>
        <div class="uk-card-body">
            • A group of actions is grouped as a transaction and the group is treated as an atomic action.<br>
            • The transaction, during the course of its execution, decides to commit or abort.<br>

            • Commit: guarantee that the transaction will be completed.<br>

            • Abort: guarantee not to do the transaction and erase any part of the transaction done so far.<br>

            • Global atomicity: (e.g.,) A distributed database transaction that must be processed at every or none of the sites.<br>

            • Commit protocols: are ones that enforce global atomicity.<br>

        </div>
    </div>

    <div id="2 phase commit protocol phase 1 two one cohorts coordinator" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>2-phase Commit Protocol</h3>
        </div>
        <div class="uk-card-body">
            • Distributed transaction carried out by a coordinator + a set of cohorts executing at different sites.<br><br>
            • Phase 1:<br>
            <b>At the coordinator:</b><br>
            - Coordinator sends a COMMIT-REQUEST message to every cohort requesting them to commit.<br>
            - Coordinator waits for reply from all others.<br>
            <b>At the cohorts:</b><br>
            - On receiving the request: if the transaction execution is successful, the cohort writes UNDO and REDO log on stable storage. Sends AGREED message to coordinator.<br>
            - Otherwise, sends an ABORT message.<br><br>
            • Phase 2:<br>
            <b>At the coordinator...:</b><br>
            - Otherwise, send an ABORT message<br>
            - Coordinator waits for acknowledgement from each cohort.<br>
            - No acknowledgement within a timeout period? : resend the commit/abort message to that cohort.<br>
            - All acknowledgements received? : write a COMPLETE record to the log.<br>
            <b>At the cohorts:</b><br>
            - On COMMIT message: resources & locks for the transaction released. Send Acknowledgement to the coordinator.<br>
            - On ABORT message: undo the transaction using UNDO log, release resources & locks held by the transaction, send Acknowledgement.<br>
        </div>
    </div>

    <div id="handling failures 2 phase commit protocol drawbacks two" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Handling failures</h3>
        </div>
        <div class="uk-card-body">
            <b>2-phase commit protocol handles failures as below:</b><br>
            • If coordinator crashes before writing the COMMIT record:<br>
            - on recovery, it will send ABORT message to all others.<br>
            - Cohorts who agreed to commit, will simply undo the transaction using the UNDO log and abort.<br>
            - Other cohorts will simply abort.<br>
            - All cohorts are blocked till coordinator recovers.<br>
            • Coordinator crashes after COMMIT before writing COMPLETE<br>
            - On recovery, broadcast a COMMIT and wait for ack<br>
            • Cohort crashes in phase 1? : coordinator aborts the transaction.<br>
            • Cohort crashes in phase 2? : on recovery, it will check with the coordinator whether to abort or commit.<br><br>
            <b>Drawback: blocking protocol. Cohorts blocked if coordinator fails.</b><br>
            • Resources and locks held unnecessarily.<br>

        </div>
    </div>

    <div id="2 phase commit state machine two drawbacks" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>2-phase commit: State Machine</h3>
        </div>
        <div class="uk-card-body">
            • Synchronous protocol: all sites proceed in rounds, i.e., a site never leads another by more than 1 state transition.<br>
            • A state transition occurs in a process participating in the 2-phase commit protocol whenever it receives/sends messages.<br>
            • States: q (idle or querying state), w (wait), a (abort), c (commit).<br>
            • When coordinator is in state q, cohorts are in q or a.<br>
            • Coordinator in w -> cohort can be in q, w, or a.<br>
            • Coordinator in a/c -> cohort is in w or a/c.<br>
            • A cohort in a/c: other cohorts may be in a/c or w.<br>
            • A site is never in c when another site is in q as the protocol is synchronous.<br><br>

            <b>Drawbacks</b><br>
            • Drawback: blocking protocol. Cohorts blocked if coordinator fails.<br>
            - Resources and locks held unnecessarily.<br>
            • Conditions that cause blocking:<br>
            - Assume that only one site is operational. This site cannot decide to abort a transaction as some other site may be in commit state.<br>
            - It cannot commit as some other site can be in abort state.<br>
            - Hence, the site is blocked until all failed sites recover.<br>
        </div>
    </div>

    <div id="non blocking commit non-blocking notations protocol buffer state phase 3 2 1 three two one" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Non-blocking Commit</h3>
        </div>
        <div class="uk-card-body">
            • Non-blocking commit:<br>
            - Sites should agree on the outcome by examining their local states.<br>
            - A failed site, upon recovery, should reach the same conclusion regarding the outcome. Consistent with other working sites.<br>
            - Independent recovery: if a recovering site can decide on the final outcome based solely on its local state.<br>
            - A non-blocking commit protocol can support independent recovery.<br><br>
            • Notations:<br>
            - Concurrency set: Let Si denote the state of the site i. The set of all the states that may be concurrent with it is concurrency set (C(si)).<br>
            - (e.g.,) Consider a system having 2 sites.If site 2’s state is w2, then C(w2) = {c1, a1, w1}. C(q2) = {q1, w1}. a1, c1 not in C(q2) as 2-phase commit protocol is synchronous within 1 state transaction.<br>
            - Sender set: Let s be any state, M be the set of all messages received in s. Sender set, S(s) = {i | site i sends m and m in M}<br><br>

            <b>Protocol:</b><br>
            Phase 1:<br>
            • First phase identical to that of 2-phase commit, except for failures.<br>
            • Here, coordinator is in w1 and each cohort is in a or w or q, depending on whether it has received the commit_request message or not.<br><br>

            Phase 2<br>
            • Coordinator sends a Prepare message to all the cohorts (if all of them sent Agreed message in phase 1).<br>
            • Otherwise, it will send an Abort message to them.<br>
            • On receiving a Prepare message, a cohort sends an acknowledgement to the coordinator.<br>
            - If the coordinator fails before sending a Prepare message, it aborts the transaction on recovery.<br>
            - Cohorts, on timing out on a Prepare message, also aborts the transaction.<br><br>

            Phase 3:<br>
            • On receiving acknowledgements to Prepare messages, the coordinator sends a Commit message to all cohorts.<br>
            • Cohort commits on receiving this message.<br>
            - Coordinator fails before sending commit? : commits upon recovery.<br>
            - So cohorts on Commit message timeout, commit to the transaction.<br>
            - Cohort failed before sending an acknowledgement? : coordinator times out and sends an abort message to all others.<br>
            - Failed cohort aborts the transaction upon recovery.<br><br>

            Use of buffer state:<br>
            • (e.g.,) Suppose state pi (in cohort) is not present. Let coordinator wait in state p1 waiting for ack. Let cohort 2 (in w2) acknowledge and commit.<br>
            • Suppose cohort 3 fails in w3. Coordinator will time out and abort. Cohort 3 will abort on recovery. Inconsistent with cohort 2.<br>
        </div>
    </div>

    <div id="3 phase commit three 3-phase" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>3-Phase Commit</h3>
        </div>
        <div class="uk-card-body">
            • Lemma: If a protocol contains a local state of a site with both abort and commit states in its concurrency set, then under independent recovery conditions it is not resilient to an arbitrary single failure.<br>
            • In previous figure, C(W2) can have both abort and commit states in the concurrency set.<br>
            • To make it a non-blocking protocol: introduce a buffer state at both coordinator and cohorts.<br>
            • Now, C(W1) = {q2, w2, a2} and C(w2) = {a1, p1, w1}.<br>
        </div>
    </div>

    <div id="failure timeout transactions rule for" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Failure, Timeout Transitions</h3>
        </div>
        <div class="uk-card-body">
            • A failure transition occurs at a failed site at the instant it fails or immediately after it recovers from the failure.<br>
            • Rule for failure transition: For every non-final state s (i.e., qi, wi, pi) in the protocol, if C(s) contains a commit, then assign a failure transition from s to a commit state in its FSA. Otherwise, assign a failure transition from s to an abort state.<br>
            • If site i is waiting on a message from j, i can time out. i can determine the state of j based on the expected message.<br>
            • Based on j’s state, the final state of j can be determined using failure transition at j.<br>
            • This can be used for incorporating Timeout transitions at i.<br>
            • Rule for timeout transition: For each nonfinal state s, if site j in S(s),and site j has a failure transition from s to a commit (abort) state, then assign a timeout transition from s to a commit (abort) state.<br>
        </div>
    </div>

    <div id="disadvantages of commit protocol" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Disadvantages of Commit Protocols</h3>
        </div>
        <div class="uk-card-body">
            No protocol using the above independent recovery technique for simultaneous failure of more than 1 site.<br>
            The above protocol is also not resilient to network partitioning.<br>
        </div>
    </div>

    <div id="basic idea of voting protocol mechanism schemes static dynamic" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Voting Protocols</h3>
        </div>
        <div class="uk-card-body">
            Basic idea of voting protocol:<br>

            Each replica assigned some number of votes<br>
            A majority of votes need to be collected before accessing a replica.<br><br>

            Voting Protocol mechanism:<br>
            More fault tolerant to site failures, network partitions, and message losses.<br><br>

            Types of voting schemes:<br>
            <b>Static</b><br>
            • System Model:<br>
            - File replicas at different sites.<br>
            - File lock rule: either one writer + no reader or multiple readers + no writer.<br>
            - Every file is associated with a version number that gives the number of times a file has been updated.<br>
            - Version numbers are stored on stable storage. Every successful write updates version number.<br><br>

            <b>Dynamic</b><br>
            • Used to overcome from failure.<br>
            • Can change vote value also.<br>
            • Dynamic voting:<br>
            • Adapt the number of votes or the set of sites that can form a quorum, to the changing state of the system due to sites & communication failures.<br>
        </div>
    </div>

    <div id="voting algorithm vote assignment" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Voting Algorithm</h3>
        </div>
        <div class="uk-card-body">
            • Let a site i issue a read or write request for a file.<br>
            • Site i issues a Lock_Request to its local lock manager<br>
            • When the lock request is granted, site i sends a Vote_Request message to all the sites<br>
            • When a site j receives a Vote_Request message, it Issues a Lock_Request to  its local lock manager.<br>
            • If the local request is granted, then it returns the version number of the replica (VNj) and the number of the votes assigned to the replica (Vj) to site i.<br>
            • If the site i is not successful in obtaining the quorum, then it issues a Release_Lock to the local lock manager as well as to all the sites in P from whom it has received votes.<br>
            • If site i is successful in obtaining the quorum, then it checks whether its copy of the file is current. A copy is current if its version number is equal to M. If the copy is not current, a current copy is obtained from a site that has a current copy. Once a current copy is available locally, site I performs the next step.<br><br>

            <b>Vote Assignment</b><br>
            Let v be the total number of votes assigned to all copies. Read & write quorum, r & w, are selected such that:<br>
            r + w > v<br>
            w > v/2<br><br>

            Above values are determined so that there is a non-null intersection between every read and write quorum, i.e., at least 1 current copy in any reading quorum gathered.<br>
            Write quorum is high enough to disallow simultaneous writes on 2 distinct subset of replicas.<br><br>

            System with 4 replicas at 4 sites. Votes assigned: V1 = 1, V2 = 1, V3 = 2, & V4 = 1.<br>
            r = 3 & w = 3<br>
        </div>
    </div>

    <div id="dynamic voting majority based approach vote reassignment" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Dynamic Voting</h3>
        </div>
        <div class="uk-card-body">
            Used to overcome from failure.<br>
            Can change vote value also.<br><br>

            Dynamic voting:<br>
            Adapt the number of votes or the set of sites that can form a quorum, to the changing state of the system due to sites & communication failures.<br><br>

            <b>Majority based approach:</b><br>

            set of sites change with system state. This set can form a majority to allow access to replicated data.<br><br>

            <b>Dynamic vote reassignment:</b><br>

            Number of votes assigned to a site changes dynamically.<br>
        </div>
    </div>

    <div id="load balancing motivation" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Motivations</h3>
        </div>
        <div class="uk-card-body">
            In a locally distributed system, there is a good possibility that several computers are heavily loaded while others are idle or lightly loaded<br>
            If we can move jobs around (in other words, distribute the load more evenly), the overall performance of the system can be maximize<br>
            A distributed scheduler is a resource management component of a distributed operating system that focuses on judiciously and transparently redistributing the load of the system among the computers to maximize the overall performance<br>
        </div>
    </div>

    <div id="issues in load balancing classification of distribution static dynamic adaptive sharing vs preemptive non-preemptive non" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>Issues In Load Distributing</h3>
        </div>
        <div class="uk-card-body">
            Load estimation<br>
            - Resource queue lengths<br>
            - CPU utilization<br><br>

            <b>Classification of Load distributing algorithms</b><br>
            • Basic function: transfer load (tasks) from heavily loaded computers to idle or lightly loaded computers.<br>
            • Can be characterized as:<br>
            - Static: decisions are hard-wired in the algorithm using a priori knowledge of the system.<br>
            - Dynamic: use system state information (the loads at nodes), at least in part.<br>
            - Adaptive: dynamically changing parameters of the algorithm to suit the changing system state.<br><br>

            <b>Load balancing vs. load sharing</b><br>
            • Strive to reduce the likelihood of an unshared state (a state in which one computer lies idle while at the same time tasks contend for service at another computer) by transferring tasks to lightly loaded nodes.<br>
            • Load balancing algorithms go a step further by attempting to equalize loads at all computers<br><br>

            <b>Preemptive vs. Non-preemptive Transfers</b><br>
            • Preemptive task transfers involve the transfer of a task that is partially executed.<br>
            • Non-preemptive task transfers involve the transfer of a task that have not begun execution.<br>

        </div>
    </div>

    <div id="keyword" class="content uk-card uk-card-default">
        <div class="uk-card-header">
            <h3>heading</h3>
        </div>
        <div class="uk-card-body">
            content
        </div>
    </div>
</div>

<script src="main.js"></script>
</body>
</html>